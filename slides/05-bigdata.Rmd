---
title: "Data Science with R"
subtitle: "Lecture 05: Big data and HPC"
author: "Christine Choirat, cchoirat@gmail.com"
date: "March 07, 2016"
output: ioslides_presentation
---

## How to deal with (very / too) large datasets?

1. Use more RAM / processors / drive space...
2. Use less data: (re)sample, ...
3. Use a database
4. Use specific R packages

https://cran.r-project.org/web/views/HighPerformanceComputing.html

http://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp

https://github.com/h2oai/h2o-2/wiki/Hacking-Airline-DataSet-with-H2O

## Airline on-time performance

```
Airlines all years 1987-2008:
https://s3.amazonaws.com/h2o-airlines-unpacked/allyears.csv (12 GB)

Or if you want something bigger here is the 10x version:
https://s3.amazonaws.com/h2o-airlines-unpacked/allyears_10.csv (120 GB)

Or if you want to play with something smaller collection of 2000 rows from all years airline data set. For this example we will be using this this data set:
https://s3.amazonaws.com/h2o-airlines-unpacked/allyears2k.csv (4.5 MB)
```

# Brute force (with 4 Gb of RAM)

`data.table`

```{r}
d <- data.table::fread("~/Downloads/allyears2k.csv")
```


Congratulations!!!  I just crashed my R session / computer.

# `bigmemory`

```{r eval=FALSE}
library(bigmemory)
library(biganalytics)

x <- read.big.matrix(
  "~/Downloads/allyears.csv", header = TRUE ,
  backingfile = "~/Downloads/airline.bin",
  descriptorfile = "~/Downloads/airline.desc"
  )
```

# `ff`

# Revolution R Open

# h2o

http://www.r-bloggers.com/scalable-machine-learning-for-big-data-using-r-and-h2o/

# AWS

# Hadoop and HDFS, Mahout

# Cloudera

# GPU
